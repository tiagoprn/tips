- Communication using ports between host->container and container->container (fig.yml):

gitlab:
  image: sameersbn/gitlab:7.6.0
  links:
   - redis:redisio
   - postgresql:postgresql
  ports:
   - "10080:80"
   - "10022:22"
  environment:
    - GITLAB_HTTPS=false
pymark:
  build: .
  command: /bin/start.sh
  volumes:
    - .:/code
  links:
    - gitlab
    - postgresql

The "ports" section on fig.yml as "10080:80" means the docker host will use port 10080 to communicate with the container, but the "pymark" container will communicate with the "gitlab" container through port 80, and NOT port 10080.

- Cleanup docker volumes:

https://github.com/chadoe/docker-cleanup-volumes

- Entering a running container:
    $ docker exec -it [myContainer] bash
  (there is no more the need to install docker-enter or use nsenter)

- Docker taking up too much space: 

	That is due to not using the "overlay (fs)" storage driver. You can check the driver being used with:

		$ docker info
	
		Check on that for "Storage Driver". If that is different from "overlay" (e.g. "devicemapper" ) - that is the reason. 

		Now you must search your distro on how to enable "overlay (fs)" with docker (linux kernel >= 3.18 required).

			- For CentOS: http://www.projectatomic.io/blog/2015/06/notes-on-fedora-centos-and-docker-storage-drivers/
				1) Enable the kernel module at boot (the conf file should just have the module name):
					$ vim /etc/modules-load.d/overlay.conf
						overlay
				2) To avoid rebooting the machine, load the kernel module manually:
					$ modprobe overlay
				3) Move on configuring overlay on docker:	
					$ lsmod | grep overlay (to check that the module was raised)
					$ systemctl stop docker
					$ rm -fr /var/lib/docker
                                        $ cp -farv /usr/lib/systemd/system/docker.service /etc/systemd/system 
					$ vim /etc/systemd/system/docker.service
						Change the line "ExecStart" to:	
							ExecStart=/usr/bin/docker daemon -H fd:// --storage-driver overlay
					$ systemctl daemon-reload
					$ systemctl start docker
					$ docker info

			- For Arch:
				1) Stop the docker service, uninstall docker, and remove its filesystem directory:
					$ systemctl stop docker
					$ pacman -Rd docker
					$ rm -fr /var/lib/docker

				2) Configure the overlay:	 
					$ vim /etc/modules-load.d/overlay.conf
						overlay
					(above will enable the kernel module at boot, available since linux kernel 3.18)
					$ modprobe overlay (enables the kernel module NOW)
					$ lsmod | grep overlay (must return something)
                                        $ cp -farv /usr/lib/systemd/system/docker.service /etc/systemd/system 
					$ vim /etc/systemd/system/docker.service
						Change the line "ExecStart" to:	
							ExecStart=/usr/bin/docker daemon -H fd:// --storage-driver overlay
					$ systemctl daemon-reload

                3) There is a recurrent bug on arch that makes the docker0 network interface not get an IP. To solve it definitely, you must do:
                    $ vim /etc/systemd/service
                    On the "After" line, add systemd-network.socket to it:
                        After=network.target docker.socket systemd-network.socket

				4) Reinstall docker:
					$ pacman -Sy
					$ pacman -S docker
					$ systemctl enable docker 
					$ systemctl start docker
					$ docker info (you must see "Storage Driver" as "overlay").
				
		OBS.: Some posts advise the use of AUFS or BTRFS. As of september/2015, many posts on the internet say both are still too unstable. 
		      CoreOS, e.g., moved from BTRFS to EXT4+OverlayFS according to https://www.phoronix.com/scan.php?page=news_item&px=CoreOS-Btrfs-To-EXT4-OverlayFS .

	Additional information:

		When you use the storage driver "device mapper", you will have one or more loopback devices (loop*) with lsblk. E.g.:

			$ sudo docker info
			Containers: 0
			Images: 0
			Storage Driver: devicemapper
			 Pool Name: docker-202:1-134217872-pool

			$ lsblk
			NAME                          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
			xvda                          202:0    0  100G  0 disk 
			└─xvda1                       202:1    0  100G  0 part /
			loop0                           7:0    0  100G  0 loop 
			└─docker-202:1-134217872-pool 253:0    0  100G  0 dm   
			loop1                           7:1    0    2G  0 loop 
			└─docker-202:1-134217872-pool 253:0    0  100G  0 dm   

		In contrast, on an Arch Linux box with the storage driver "overlay", you will see no loopback devices. E.g.:

			$ docker info
			Containers: 35
			Images: 241
			Storage Driver: overlay
			 Backing Filesystem: extfs

			$ lsblk
			NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
			sda      8:0    0 931,5G  0 disk 
			├─sda1   8:1    0 199,2G  0 part /
			├─sda2   8:2    0     1K  0 part 
			├─sda3   8:3    0 199,2G  0 part /crypted
			├─sda4   8:4    0 527,2G  0 part /vault
			└─sda5   8:5    0   5,9G  0 part 
			sr0     11:0    1  1024M  0 rom  


	Reasons to prefer overlay: Faster, more reliable, and it seems to be what the Docker core team are running on their own setups, so it’s likely the one that’s getting the most testing.
	It also is in the Linux kernel proper as of 3.18. AUFS, on the other hand, was an out-of-tree kernel patchset, so each distro would have to evaluate if they would maintain it and put it into their kernel builds.

---

PUSH TO DOCKERHUB:

Run your image with "compose up":
	$ compose up -d

Get the CONTAINER_ID:
	$ docker ps | grep "your container name here"

If it is your first push on the machine, you must login first:
	$ docker login

Commit the container ID and assign an image name to it:
	$ docker commit -m "commit message" -a "your user name" [CONTAINER_ID] [YOUR_USER_NAME]/[YOUR_IMAGE_NAME]:version_number.
	E.g.:
		$ docker commit -m "Added json gem" -a "Kate Smith" 0b2616b0e5a8 ouruser/sinatra:v2

Push your image to DockerHub:
	$ docker push [YOUR_USER_NAME]/[YOUR_IMAGE_NAME]
	E.g.:
		$ docker push ouruser/sinatra

---

BUILDING / TRANSPORTING IMAGES USING TAR FILES (ALTERNATIVE TO USING DOCKERHUB):

The idea is to generate an image of the container, save it to a tar file, copy the tar file to the new host and load the image on the new docker server. 

IF YOU WILL *BUILD A NEW* IMAGE:

	1) Enter the directory that contains the Dockerfile:
	    $ cd /path/to/Dockerfile	

	2) Build the new image:
            A) TAG as the new incremental version
		    $ docker build -t [YOUR_USER_NAME]/[DESIRED_IMAGE_NAME]:version_number .
            B) TAG as "latest" (good practice, it will not waste time and space because it will use the cache of "version_number", and tag as "latest"):
		    $ docker build -t [YOUR_USER_NAME]/[DESIRED_IMAGE_NAME]:latest .

IF YOU WILL *UPDATE AN EXISTING* IMAGE:

	1) Find the id of the container
	    $ docker ps

	2) Commit the changes you made to the container:
	    $ docker commit [CONTAINER_ID] [DESIRED_IMAGE_NAME]:version_number

3) Save the image to a tar file:
    $ docker save [DESIRED_IMAGE_NAME] > /tmp/[DESIRED_IMAGE_NAME].tar

4) Zip the tar file. Copy it to its new destination machine using scp or anything you like. 
    $ tar cfjv /tmp/[DESIRED_IMAGE_NAME].tar.bz2 /tmp/[DESIRED_IMAGE_NAME].tar
    $ scp /tmp/[DESIRED_IMAGE_NAME].tar.bz2 user@remote_host:/tmp

5) Log to the remote_host and uncompress the file you copied to it:
    $ ssh user@remote_host
    $ cd /tmp && tar xfjv /tmp/[DESIRED_IMAGE_NAME].tar.bz2

6) Load the uncompressed file into docker:
    $ docker load < /tmp/[DESIRED_IMAGE_NAME].tar
   
7) Check the image was loaded correctly:
    $ docker images
    (you will see the image there alonside the others)

---

To run docker exec from crontab (e.g., to enable backups from postgres), I have to do the following:
    - On /etc/users, comment "requiretty". 
    - Do not use "docker run -it", but just "docker run -i".

E.g.: 

$ vim pg_dump.sh

```
#!/bin/bash
HOST="localhost"
PORT=5432
USERNAME="postgres"
PASSWORD="v3t3x2y2"
DATABASE="postgres"
ROLE=$USERNAME
FORMAT="c"
CONTAINERFILE="/BACKUPS/${DATABASE}.${FORMAT}.backup"
HOSTFILE="/vault/BACKUPS/databases/taiga/${DATABASE}.${FORMAT}.backup"

TIMESTAMP=$(date +%Y%m%d-%H%M%S-%N)
TARFILE="/vault/BACKUPS/databases/taiga/${TIMESTAMP}.tar.bz2"

sudo docker exec -i $(sudo docker ps | grep 'postgres'| awk '{print $1}') pg_dump --username $USERNAME --role $ROLE --format $FORMAT --verbose --file $CONTAINERFILE $DATABASE

sudo tar cfjv $TARFILE $HOSTFILE
sudo chown tiago.users $TARFILE
sudo chmod 777 $TARFILE
sudo rm $HOSTFILE
```

---

How to run a container accessing the host on its entirety (useful for zabbix and others):
    $ docker run --rm -it --privileged --pid=host --net=host ...

---

How to build a docker container that runs multiple commands and keeps showing the logs: 


seafile:
  # build: .
  image: tiagoprn/seafile
  ports:
    - 8082:8082
    - 8000:8000
  command: bash -c "./start.sh && tail -f /seafile/logs/seahub.log"


PS: The tail command below can also be used to keep a container running forever, 
if I pass a file that exists on the container. 

---


